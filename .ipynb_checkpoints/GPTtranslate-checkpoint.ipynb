{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "id": "e60ffc67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import openai\n",
        "import os\n",
        "import requests\n",
        "import bs4"
      ],
      "id": "3f6f3c08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.environ['API_KEY'] = \"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\""
      ],
      "id": "f38c625e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "openai.api_key = os.getenv(\"API_KEY\")"
      ],
      "id": "e2f82d9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "news_sites = {\n",
        "    \"chinese\" : (\"https://cn.chinadaily.com.cn\", \"div.Home_content_Item_Text h1 a\"),\n",
        "    \"arabic\": (\"https://aljazeera.net\", \"h3.article-card__title\")\n",
        "}\n",
        "print(news_sites[\"arabic\"])"
      ],
      "id": "9d74133e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##DO NOT USE CELL BELOW\n"
      ],
      "id": "43bc5d8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# user_language = input(\"In what language are you interested in getting your news?\")\n",
        "# selected_url = news_sites.get(user_language, \"Language not supported\")\n",
        "# print(selected_url)"
      ],
      "id": "8c77d85d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# def fetch_headlines(language): \n",
        "#     url, tag = news_sites.get(language, (None, None))\n",
        "#     response = requests.get(url)\n",
        "#     soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
        "#     headlines = [h.getText() for h in soup.select(tag)[:10]]\n",
        "#     return headlines\n",
        "# selected_headlines = fetch_headlines(user_language)\n",
        "# print(selected_headlines)"
      ],
      "id": "6dd04458",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fetch_headlines():\n",
        "    headlines_by_language = {}\n",
        "    for language, (url, tag) in news_sites.items():\n",
        "        response = requests.get(url)\n",
        "        soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
        "        headlines = [h.getText() for h in soup.select(tag)[:10]]\n",
        "        headlines_by_language[language] = headlines\n",
        "    return headlines_by_language\n",
        "fetch_headlines()"
      ],
      "id": "1accf0fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def translate_headlines(headlines_by_language):\n",
        "    translated_headlines = {}\n",
        "    for language, headlines in headlines_by_language.items():\n",
        "        translated_headlines[language] = []\n",
        "        for headline in headlines:\n",
        "            response = openai.Completion.create(\n",
        "                model=\"gpt-3.5-turbo-instruct\",\n",
        "                prompt=f\"Translate the following {language} headline into English: \\\"{headline}\\\"\",\n",
        "                temperature=0.3,\n",
        "                max_tokens=60\n",
        "            )\n",
        "            translated_headline = response.choices[0].text.strip()\n",
        "            translated_headlines[language].append(translated_headline)\n",
        "    return translated_headlines\n",
        "headlines_by_language = fetch_headlines()\n",
        "translated_headlines = translate_headlines(headlines_by_language)"
      ],
      "id": "26cf6459",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(translated_headlines)"
      ],
      "id": "5611e60e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(translated_headlines)"
      ],
      "id": "84d9f9db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def analyze_sentiment(translated_headlines):\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "    sentiment_results = {}\n",
        "    \n",
        "    for language, headlines in translated_headlines.items():\n",
        "        language_results = []\n",
        "        for headline in headlines:\n",
        "            sentiment = sentiment_pipeline(headline)\n",
        "            sentiment_result = {\n",
        "                'text': headline,\n",
        "                'label': sentiment[0]['label'],\n",
        "                'score': sentiment[0]['score']\n",
        "            }\n",
        "            language_results.append(sentiment_result)\n",
        "        sentiment_results[language] = language_results\n",
        "    \n",
        "    return sentiment_results\n",
        "\n",
        "# Example usage:\n",
        "# translated_headlines = translate_headlines(headlines_by_language)\n",
        "# sentiment_analysis_results = analyze_sentiment(translated_headlines)"
      ],
      "id": "3629016a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def analyze_sentiment_and_print(translated_headlines):\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "    \n",
        "    for language, headlines in translated_headlines.items():\n",
        "        print(f\"Language: {language}\")\n",
        "        for headline in headlines:\n",
        "            sentiment = sentiment_pipeline(headline)\n",
        "            print(f\"Headline: '{headline}'\")\n",
        "            print(f\"Label: {sentiment[0]['label']}, Score: {sentiment[0]['score']:.2f}\")\n",
        "            print(\"---\")  # Separator for readability\n",
        "\n",
        "# Assuming translated_headlines is your dictionary of translated headlines\n",
        "analyze_sentiment_and_print(translated_headlines)"
      ],
      "id": "238f10bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#turn each set of translated headlines into a dataframe that can be saved as CSV  \n",
        "#consider AUTOMATING the scraping and translating every hour (GitHub Actions)\n",
        "#visualize "
      ],
      "id": "2afaf31a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#pseudocode:\n",
        "# \"fetch_headlines\" function should take the news_sites object\n",
        "# then take the first part of the tuple for each key-value pair \n",
        "# for url in news_list:\n",
        "# response = requests.get(url)\n",
        "#   soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
        "#   headlines = [h.getText() for h in soup.select(tag)[:10]]\n",
        "#   return headlines with the language (key) prepended "
      ],
      "id": "66bc32c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#let's write the function to LOOP thru the news_sites, find the headlines and SPIT OUT several arrays of headlines \n",
        "# def fetch_headlines(news_sites): \n",
        "#     url, tag = news_sites.get(language, (None, None))\n",
        "#     if not url:\n",
        "#         print('language not supported')\n",
        "#         return\n",
        "#     response = requests.get(url)\n",
        "#     soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "#     headlines = [h.getText() for h in soup.select(tag)[:10]]\n",
        "#     return headlines\n",
        "# selected_headlines = fetch_headlines(user_language)\n",
        "# #print(selected_headlines)"
      ],
      "id": "4ff7ac10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#translate headlines into English using OpenAI\n",
        "def create_prompt(headlines):\n",
        "    joined_headlines = \"\\n\".join(headlines)\n",
        "    prompt = f\"Translate the following headlines into English: \\n{joined_headlines}\"\n",
        "    return prompt\n",
        "print(create_prompt(selected_headlines))"
      ],
      "id": "e00156c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = create_prompt(selected_headlines)\n",
        "response = openai.Completion.create(\n",
        "    model = 'gpt-3.5-turbo-instruct',\n",
        "    prompt = prompt, \n",
        "    temperature = 0.1,\n",
        "    max_tokens =200\n",
        ")\n",
        "\n",
        "#previously used: text-davinci-003\n",
        "#then: gpt-3.5-turbo-instruct\n",
        "#then: gpt-4 -- not appropriate \n",
        "\n",
        "#temp == the output should be similar in tone to the original\n",
        "body = response['choices'][0]['text']\n",
        "type(body)\n",
        "print(body)"
      ],
      "id": "5b7c6af5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# What I'm going to work next: \n",
        "# Find free websites in German, French, Spanigh, Russian\n",
        "# Create openai prompt to find tags\n",
        "# Test the prompts "
      ],
      "id": "054935ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# What I'm going to work next: \n",
        "# Find free websites in German, French, Spanish, Russian\n",
        "# Translate all the main headlines \n",
        "# Do a sentiment analysis on the headlines using a LLM\n",
        "# calculate the avg and median scores for each language\n",
        "#COMPARE the total score for each language's website "
      ],
      "id": "f72340a5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}