{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96918b7-911a-4be1-94c1-7c7b064bcd22",
   "metadata": {},
   "source": [
    "### Translating and Comparing News Headlines: step by step  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f814d4a-a9ab-4c5a-894e-012b2575cb60",
   "metadata": {},
   "source": [
    "#### Comparing headlines across languages is cumbersome; doing an automated sentiment analysis on a sampling used to be very complex before the advent of ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c73e76c-f5d4-47eb-9b41-944803b290c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "# import textwrap\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import google.generativeai as genai\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# def to_markdown(text):\n",
    "#   text = text.replace('•', '  *')\n",
    "#   return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963617c3-fbca-4038-81b9-924a2083a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import os\n",
    "# import requests\n",
    "# import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b69a95-b43c-470d-9a69-898f50daa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6316da-c74c-4a7b-85c6-cc5c4de6077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc1a8b3-9d3f-43a0-979e-e82f37b94e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://aljazeera.net', 'h3.article-card__title')\n"
     ]
    }
   ],
   "source": [
    "news_sites = {\n",
    "    \"chinese\" : (\"https://cn.chinadaily.com.cn\", \"div.Home_content_Item_Text h1 a\"),\n",
    "    \"arabic\": (\"https://aljazeera.net\", \"h3.article-card__title\")\n",
    "}\n",
    "print(news_sites[\"arabic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96c430a-b70e-432b-9358-cb31659b92fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chinese': ['《中国日报》专访最高人民法院院长张军：促进提升治理效能 以公权力为网暴受害者撑腰',\n",
       "  '《中国日报》专访最高人民检察院检察长应勇：坚持电信网络诈骗犯罪\"打防管控\"一体推进 深化国际执法司法合作',\n",
       "  '甘肃省委书记、省人大常委会主任胡昌升：以一流营商环境支撑经济社会高质量发展',\n",
       "  '刘结一：应推动谈判解决地区冲突，不要火上浇油',\n",
       "  '涉及教育、就业、社会保障等民生问题 这场记者会事关你我',\n",
       "  '刘结一：美国应兑现在中美关系上的承诺',\n",
       "  '中科院院士刘忠范：企业必须是创新的主体',\n",
       "  '全国政协委员贺晗：加快推进大模型技术与传统产业深度融合',\n",
       "  '最高检：公益诉讼检察办案效果凸显 清理生产类固体废物528.9万吨',\n",
       "  '民生主题记者会丨人社部：社保卡将逐步实现“全国一卡通”'],\n",
       " 'arabic': ['الحرب على غزة.. قصف مكثف وسط القطاع والقسام تغنم طائرتي “كواد كابتر”',\n",
       "  'الاحتلال يقر بمقتل ضابط بمعارك غزة والقسام تعلن السيطرة على مسيرتين',\n",
       "  'الاحتلال يقر بمقتل ضابط بمعارك غزة والقسام تعلن السيطرة على مسيرتين',\n",
       "  'الشرطة تعتقل 10 بمظاهرات إسرائيلية حاشدة تطالب بصفقة تبادل ورحيل نتنياهو',\n",
       "  'الموساد و\"سي آي إيه\" يبحثان جهود التوصل لصفقة مع حماس',\n",
       "  'تفاصيل عن تصفية 50 مدنيا.. الجزيرة تكشف هوية مسن أعزل أعدمه الاحتلال ...',\n",
       "  'شابة ورضيعة تلتحقان بشهداء الجوع في قطاع غزة',\n",
       "  'هنية يناشد العرب والمسلمين سرعة التحرك لإغاثة غزة وعزل إسرائيل',\n",
       "  'مبارزة انتخابية بين بايدن وترامب في ولاية الحسم',\n",
       "  'السلطة الفلسطينية: موجز تاريخ الانحدار']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_headlines():\n",
    "    headlines_by_language = {}\n",
    "    for language, (url, tag) in news_sites.items():\n",
    "        response = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "        headlines = [h.getText() for h in soup.select(tag)[:10]]\n",
    "        headlines_by_language[language] = headlines\n",
    "    return headlines_by_language\n",
    "fetch_headlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889344ff-da73-499f-b1bc-5e28fc47cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_headlines(headlines_by_language):\n",
    "    translated_headlines = {}\n",
    "    for language, headlines in headlines_by_language.items():\n",
    "        translated_headlines[language] = []\n",
    "        for headline in headlines:\n",
    "            response = openai.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=f\"Translate the following {language} headline into English: \\\"{headline}\\\"\",\n",
    "                temperature=0.3,\n",
    "                max_tokens=60\n",
    "            )\n",
    "            translated_headline = response.choices[0].text.strip()\n",
    "            translated_headlines[language].append(translated_headline)\n",
    "    return translated_headlines\n",
    "headlines_by_language = fetch_headlines()\n",
    "translated_headlines = translate_headlines(headlines_by_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aec9adb-8545-4c23-a613-5a714b421669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chinese': ['\"China Daily Interviews Supreme People\\'s Court President Zhang Jun: Promoting Improved Governance Efficiency and Supporting Victims of Online Bullying with State Power\"', '\"China Daily Interviews Procurator-General of Supreme People\\'s Procuratorate, Ying Yong: Insisting on Combating and Controlling Telecommunications and Internet Fraud, Deepening International Law Enforcement and Judicial Cooperation\"', '\"Gansu Provincial Party Secretary and Chairman of the Provincial People\\'s Congress Standing Committee, Hu Changsheng: Supporting High-Quality Economic and Social Development with First-Class Business Environment\"', 'Liu Jieyi: Negotiations should be promoted to resolve regional conflicts, not add fuel to the fire.', '\"Press Conference Concerning Livelihood Issues Involving Education, Employment, and Social Security Affects You and Me\"', 'Liu Jieyi: The United States should fulfill its commitments in US-China relations', '\"Academician Liu Zhongfan of Chinese Academy of Sciences: Enterprises Must Be the Main Force of Innovation\"', '\"National Committee Member of the Chinese People\\'s Political Consultative Conference, He Han: Accelerating the Deep Integration of Large-Scale Model Technology and Traditional Industries\"', '\"Supreme Procuratorate: Outstanding Results in Public Interest Litigation Cases, 5.289 Million Tons of Solid Waste from Production Cleared\"', '\"Ministry of Human Resources and Social Security: Social Security Card to Gradually Achieve \"National One-Card\" System\"'], 'arabic': ['\"War on Gaza: Intensive Bombardment in the Center of the Strip and Hamas Seizes Two \"Quadcopter\" Drones\"', '\"Occupation Acknowledges Officer\\'s Death in Gaza Battles, Al-Qassam Declares Control Over Two Marches\"', '\"Occupation admits to killing an officer in Gaza battles and Al-Qassam announces control over two marches\"', '\"Police Arrest 10 at Massive Israeli Protests Demanding a Prisoner Exchange Deal and Netanyahu\\'s Departure\"', '\"Mossad and CIA Discuss Efforts to Reach a Deal with Hamas\"', '\"Details about the killing of 50 civilians... Al Jazeera reveals the identity of an elderly isolated man executed by the occupation...\"', '\"Young Woman and Infant Join the Martyrs of Hunger in Gaza Strip\"', '\"Haniyeh urges Arabs and Muslims to act quickly to aid Gaza and isolate Israel\"', '\"Election Battle Between Biden and Trump in the Decisive State\"', '\"Palestinian Authority: A Brief History of Decline\"']}\n"
     ]
    }
   ],
   "source": [
    "print(translated_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14530342-b9a7-4dc0-8f74-c9cc2009b205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a25dfa-7bd0-4a79-ab18-499f7565451f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ead39-2941-4509-872d-095c603cb6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f0e6b-22c4-46cb-81c3-2f34d29f2e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b158f66-b28d-44e1-8eee-308c813c53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_sentiment_and_score_to_dataframes(translated_headlines):\n",
    "#   sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "#   chinese_data = []\n",
    "#   arabic_data = []\n",
    "\n",
    "#   for language, headlines in translated_headlines.items():\n",
    "#     # Count sentiment categories\n",
    "#     positive_count = 0\n",
    "#     negative_count = 0\n",
    "#     neutral_count = 0\n",
    "\n",
    "#     # Analyze each headline\n",
    "#     for headline in headlines:\n",
    "#       sentiment = sentiment_pipeline(headline)\n",
    "#       label = sentiment[0]['label']\n",
    "#       score = sentiment[0]['score']\n",
    "\n",
    "#       # Assign custom score based on label and score\n",
    "\n",
    "#       if label == 'negative':\n",
    "#         if score > 0.7:\n",
    "#           categ_score = -2\n",
    "#         else:\n",
    "#           categ_score = -1\n",
    "#         negative_count += 1\n",
    "#       elif label == 'positive':\n",
    "#         if score > 0.7:\n",
    "#           categ_score = 3\n",
    "#         else:\n",
    "#           categ_score = 2\n",
    "#         positive_count += 1\n",
    "#       else:\n",
    "#         categ_score = 1\n",
    "#         neutral_count += 1\n",
    "\n",
    "#       # Create a dictionary for each headline and its analysis\n",
    "#       result = {\n",
    "#           'language': language,\n",
    "#           \"text\": headline,\n",
    "#           \"label\": label,\n",
    "#           \"score\": score,\n",
    "#           \"categ_score\": categ_score\n",
    "#       }\n",
    "\n",
    "#       # Append data based on language\n",
    "#       if language == 'chinese':\n",
    "#         chinese_data.append(result)\n",
    "#       else:\n",
    "#         arabic_data.append(result)\n",
    "\n",
    "#     # Create DataFrames and add sentiment counts\n",
    "#     chinese_df = pd.DataFrame(chinese_data)\n",
    "#     chinese_df['positive_count'] = positive_count\n",
    "#     chinese_df['negative_count'] = negative_count\n",
    "#     chinese_df['neutral_count'] = neutral_count\n",
    "\n",
    "#     arabic_df = pd.DataFrame(arabic_data)\n",
    "#     arabic_df['positive_count'] = positive_count\n",
    "#     arabic_df['negative_count'] = negative_count\n",
    "#     arabic_df['neutral_count'] = neutral_count\n",
    "\n",
    "#     # Print sentiment category counts as formatted strings\n",
    "#     print(f\"{language} headlines: {positive_count} positive, {negative_count} negative, {neutral_count} neutral\")\n",
    "\n",
    "#   return chinese_df, arabic_df\n",
    "\n",
    "\n",
    "# # Assuming translated_headlines is your dictionary of translated headlines\n",
    "# chinese_df, arabic_df = analyze_sentiment_and_score_to_dataframes(translated_headlines)\n",
    "# print(chinese_df)\n",
    "# print(arabic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe18ba1-7d93-46d0-a0c9-2a68a457efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chinese_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0da241-2ae7-4b6e-999c-375708c470f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e77641-d475-4179-bc54-4b10947ebcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c845967-8772-477b-bc92-355af3615d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_and_score(translated_headlines):\n",
    "  sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "  data = []\n",
    "\n",
    "  for language, headlines in translated_headlines.items():\n",
    "    # Count sentiment categories\n",
    "    positive = 0\n",
    "    negative  = 0\n",
    "    neutral = 0\n",
    "\n",
    "    # Analyze each headline\n",
    "    for headline in headlines:\n",
    "      sentiment = sentiment_pipeline(headline)\n",
    "      label = sentiment[0]['label']\n",
    "      score = sentiment[0]['score']\n",
    "\n",
    "      # Assign custom score based on label and score\n",
    "\n",
    "      if label == 'negative':\n",
    "        if score > 0.7:\n",
    "          categ_score = -2\n",
    "        else:\n",
    "          categ_score = -1\n",
    "        negative += 1\n",
    "      elif label == 'positive':\n",
    "        if score > 0.7:\n",
    "          categ_score = 3\n",
    "        else:\n",
    "          categ_score = 2\n",
    "        positive += 1\n",
    "      else:\n",
    "        categ_score = 1\n",
    "        neutral += 1\n",
    "\n",
    "      # Create a dictionary for each headline and its analysis\n",
    "      result = {\n",
    "          'language': language,\n",
    "          \"text\": headline,\n",
    "          \"label\": label,\n",
    "          \"score\": score,\n",
    "          \"categ_score\": categ_score\n",
    "      }\n",
    "      data.append(result)\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "data = analyze_sentiment_and_score(translated_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81709ba-f7a3-4c0f-aa03-ba00dbf12ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aabd6ad3-b8aa-4011-bf33-69e2222361ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'chinese', 'text': '\"China Daily Interviews Supreme People\\'s Court President Zhang Jun: Promoting Improved Governance Efficiency and Supporting Victims of Online Bullying with State Power\"', 'label': 'neutral', 'score': 0.5576910376548767, 'categ_score': 1}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922ae07-1c29-497b-9c7f-cbef65cd97b9",
   "metadata": {},
   "source": [
    "### Scoping issue with \"print\" command below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf82d1a9-c413-4e04-ae6e-ff8f7afdb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_df_by_language(data):\n",
    "#     # Separate data by language\n",
    "#     chinese_data = [row for row in data if row['language'] == 'chinese']\n",
    "#     arabic_data = [row for row in data if row['language'] == 'arabic']\n",
    "\n",
    "#     # Create DataFrames and add sentiment counts\n",
    "#     chinese_df = pd.DataFrame(chinese_data)\n",
    "#     chinese_df['positive'] = len([row for row in chinese_data if row['label'] == 'positive'])\n",
    "#     chinese_df['negative'] = len([row for row in chinese_data if row['label'] == 'negative'])\n",
    "#     chinese_df['neutral'] = len([row for row in chinese_data if row['label'] == 'neutral'])\n",
    "\n",
    "#     arabic_df = pd.DataFrame(arabic_data)\n",
    "#     arabic_df['positive'] = len([row for row in arabic_data if row['label'] == 'positive'])\n",
    "#     arabic_df['negative'] = len([row for row in arabic_data if row['label'] == 'negative'])\n",
    "#     arabic_df['neutral'] = len([row for row in arabic_data if row['label'] == 'neutral'])\n",
    "\n",
    "#     # Print sentiment category counts as formatted strings\n",
    "#     print(f\"Chinese headlines: {chinese_df['positive'].iloc[0]} positive, {chinese_df['negative'].iloc[0]} negative, {chinese_df['neutral'].iloc[0]} neutral\")\n",
    "#     print(f\"Arabic headlines: {arabic_df['positive'].iloc[0]} positive, {arabic_df['negative'].iloc[0]} negative, {arabic_df['neutral'].iloc[0]} neutral\")\n",
    "\n",
    "#     return chinese_df, arabic_df\n",
    "\n",
    "# chinese_df, arabic_df = create_df_by_language(data)\n",
    "# print(\"DATAFRAME\")\n",
    "# # print(chinese_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2926178-9998-4cf8-a40f-3b4af0dee442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81e4814e-b2c3-41ec-bc98-7e7018986630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINESE DATAFRAME:\n",
      "  language                                               text     label  \\\n",
      "0  chinese  \"China Daily Interviews Supreme People's Court...   neutral   \n",
      "1  chinese  \"China Daily Interviews Procurator-General of ...   neutral   \n",
      "2  chinese  \"Gansu Provincial Party Secretary and Chairman...  positive   \n",
      "3  chinese  Liu Jieyi: Negotiations should be promoted to ...   neutral   \n",
      "4  chinese  \"Press Conference Concerning Livelihood Issues...   neutral   \n",
      "5  chinese  Liu Jieyi: The United States should fulfill it...   neutral   \n",
      "6  chinese  \"Academician Liu Zhongfan of Chinese Academy o...   neutral   \n",
      "7  chinese  \"National Committee Member of the Chinese Peop...   neutral   \n",
      "8  chinese  \"Supreme Procuratorate: Outstanding Results in...  positive   \n",
      "9  chinese  \"Ministry of Human Resources and Social Securi...   neutral   \n",
      "\n",
      "      score  categ_score  positive  negative  neutral  \n",
      "0  0.557691            1         2         0        8  \n",
      "1  0.595754            1         2         0        8  \n",
      "2  0.574793            2         2         0        8  \n",
      "3  0.449128            1         2         0        8  \n",
      "4  0.808578            1         2         0        8  \n",
      "5  0.455775            1         2         0        8  \n",
      "6  0.625868            1         2         0        8  \n",
      "7  0.676334            1         2         0        8  \n",
      "8  0.639399            2         2         0        8  \n",
      "9  0.591038            1         2         0        8  \n",
      "ARABIC DATAFRAME:\n",
      "Empty DataFrame\n",
      "Columns: [positive, negative, neutral]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def create_df_by_language(data):\n",
    "    # Separate data by language\n",
    "    chinese_data = [row for row in data if row['language'] == 'chinese']\n",
    "    arabic_data = [row for row in data if row['language'] == 'arabic']\n",
    "\n",
    "    # Create DataFrames and add sentiment counts\n",
    "    chinese_df = pd.DataFrame(chinese_data)\n",
    "    chinese_df['positive'] = len([row for row in chinese_data if row['label'] == 'positive'])\n",
    "    chinese_df['negative'] = len([row for row in chinese_data if row['label'] == 'negative'])\n",
    "    chinese_df['neutral'] = len([row for row in chinese_data if row['label'] == 'neutral'])\n",
    "\n",
    "    arabic_df = pd.DataFrame(arabic_data)\n",
    "    arabic_df['positive'] = len([row for row in arabic_data if row['label'] == 'positive'])\n",
    "    arabic_df['negative'] = len([row for row in arabic_data if row['label'] == 'negative'])\n",
    "    arabic_df['neutral'] = len([row for row in arabic_data if row['label'] == 'neutral'])\n",
    "\n",
    "    # Return the DataFrames\n",
    "    return chinese_df, arabic_df\n",
    "\n",
    "# Call the function and assign the returned DataFrames\n",
    "chinese_df, arabic_df = create_df_by_language(data)\n",
    "\n",
    "# Print the DataFrames after they are created\n",
    "print(\"CHINESE DATAFRAME:\")\n",
    "print(chinese_df)\n",
    "print(\"ARABIC DATAFRAME:\")\n",
    "print(arabic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1035d2-f42b-41b1-a3a8-9c459c51d753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f344e85-6b21-43d0-9de7-4c71b964fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83e92d-549c-4faf-b364-9525c352c2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9dea304-c6ec-4256-ae20-9c0a6823947c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language                                               text     label  \\\n",
      "0  chinese  \"China Daily Interviews Supreme People's Court...   neutral   \n",
      "1  chinese  \"China Daily Interviews Procurator-General of ...   neutral   \n",
      "2  chinese  \"Gansu Provincial Party Secretary and Chairman...  positive   \n",
      "3  chinese  Liu Jieyi: Negotiations should be promoted to ...   neutral   \n",
      "4  chinese  \"Press Conference Concerning Livelihood Issues...   neutral   \n",
      "5  chinese  Liu Jieyi: The United States should fulfill it...   neutral   \n",
      "6  chinese  \"Academician Liu Zhongfan of Chinese Academy o...   neutral   \n",
      "7  chinese  \"National Committee Member of the Chinese Peop...   neutral   \n",
      "8  chinese  \"Supreme Procuratorate: Outstanding Results in...  positive   \n",
      "9  chinese  \"Ministry of Human Resources and Social Securi...   neutral   \n",
      "\n",
      "      score  categ_score  positive  negative  neutral  \n",
      "0  0.557691            1         2         0        8  \n",
      "1  0.595754            1         2         0        8  \n",
      "2  0.574793            2         2         0        8  \n",
      "3  0.449128            1         2         0        8  \n",
      "4  0.808578            1         2         0        8  \n",
      "5  0.455775            1         2         0        8  \n",
      "6  0.625868            1         2         0        8  \n",
      "7  0.676334            1         2         0        8  \n",
      "8  0.639399            2         2         0        8  \n",
      "9  0.591038            1         2         0        8  \n"
     ]
    }
   ],
   "source": [
    "print(chinese_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96e07036-aa1a-44dd-9cab-9869044acc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_df.to_csv(\"chinese.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84feff02-9b35-4c9b-8b0c-9ed5825b1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_df.to_csv(\"arabic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fde5a8b-265f-4143-8c58-8509a85231b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "GPTdf-bylang_count_totals.ipynb  README.md\n",
      "GPTdf-bylanguage.ipynb           action.yml\n",
      "GPTdf-gemini.ipynb               arabic.csv\n",
      "GPTdf-manual.ipynb               chinese.csv\n",
      "GPTdf-russian.ipynb              clean_GPTscrape.ipynb\n",
      "GPTscrape.ipynb                  home.html\n",
      "GPTtrans-sentiment.ipynb         index.html\n",
      "GPTtrans-sentiment.qmd           old_index.html\n",
      "\u001b[1m\u001b[36mGPTtrans-sentiment_files\u001b[m\u001b[m/        results.csv\n",
      "GPTtranslate.ipynb               russian.csv\n",
      "GPTviz-gemini.ipynb              wider_GPTscrape.ipynb\n",
      "GPTviz-mistral.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397ac3b-64b2-4c91-9f84-04d88af81f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ced58f6-b3f8-4470-ba54-ed8fc0570037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# def analyze_sentiment_and_get_results(translated_headlines):\n",
    "#   sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "#   results = []\n",
    "\n",
    "#   for language, headlines in translated_headlines.items():\n",
    "#     for headline in headlines:\n",
    "#       sentiment = sentiment_pipeline(headline)\n",
    "#       # Extract label and score\n",
    "\n",
    "#       label = sentiment[0]['label']\n",
    "#       score = sentiment[0]['score']\n",
    "\n",
    "#     # return analyzed_headlines\n",
    "        \n",
    "#       # Create a dictionary for each headline and its analysis\n",
    "#       result = {\n",
    "#         'language': language,\n",
    "#           \"text\": headline,\n",
    "#           \"label\": label,\n",
    "#           \"score\": score,\n",
    "#       }\n",
    "#       results.append(result)\n",
    "#   return results\n",
    "\n",
    "# # Assuming translated_headlines is your dictionary of translated headlines\n",
    "# results = analyze_sentiment_and_get_results(translated_headlines)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e25a00-8a46-4193-a704-94eb5ace7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in results: \n",
    "#     if row['label'] == 'negative':\n",
    "#         if row['score'] > 0.7:\n",
    "#             row['categ_score'] = -2\n",
    "#         else: \n",
    "#             row['categ_score'] = -1\n",
    "#     elif row['label'] == 'positive':\n",
    "#         if row['score'] > 0.7:\n",
    "#             row['categ_score'] = 3\n",
    "#         else:\n",
    "#             row['categ_score'] = 2\n",
    "#     else:\n",
    "#         row['categ_score'] = 1\n",
    "# print(results[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e54316-8c86-4562-a138-58c2ca1f1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "\n",
    "# chinese_data = []\n",
    "# arabic_data = []\n",
    "\n",
    "# # Iterate through the results and add elements to respective lists\n",
    "# for item in results:\n",
    "#   if item[\"language\"] == \"chinese\":\n",
    "#     chinese_data.append(item)\n",
    "#   elif item[\"language\"] == \"arabic\":\n",
    "#     arabic_data.append(item)\n",
    "\n",
    "# # Create DataFrames from lists\n",
    "# chinese_df = pd.DataFrame(chinese_data)\n",
    "# arabic_df = pd.DataFrame(arabic_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7683033-80d0-47f9-b744-16f0e04d9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the DataFrames\n",
    "# print(\"Chinese headlines:\")\n",
    "# print(chinese_df)\n",
    "# print(\"\\nArabic headlines:\")\n",
    "# print(arabic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb2f57da-4176-4bdd-9881-d8eeb65823d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese_df.to_csv(\"chinese.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fadeb916-f1df-40c4-ba6e-08d0b32607c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabic_df.to_csv(\"arabic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5195911e-8956-4151-a340-66d624e0ecf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "GPTdf-bylang_count_totals.ipynb  README.md\n",
      "GPTdf-bylanguage.ipynb           action.yml\n",
      "GPTdf-gemini.ipynb               arabic.csv\n",
      "GPTdf-manual.ipynb               chinese.csv\n",
      "GPTdf-russian.ipynb              clean_GPTscrape.ipynb\n",
      "GPTscrape.ipynb                  home.html\n",
      "GPTtrans-sentiment.ipynb         index.html\n",
      "GPTtrans-sentiment.qmd           old_index.html\n",
      "\u001b[1m\u001b[36mGPTtrans-sentiment_files\u001b[m\u001b[m/        results.csv\n",
      "GPTtranslate.ipynb               russian.csv\n",
      "GPTviz-gemini.ipynb              wider_GPTscrape.ipynb\n",
      "GPTviz-mistral.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5f00c5b-43d3-456e-a8bc-c2bb85a4acb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marabic_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# arabic_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a498be-fa04-4ebd-affa-3a71685ca11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c34d5-9c9a-436c-b316-056b141b7bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b186aa2-5456-4335-ae86-7715c5e2ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# def analyze_sentiment_and_print(translated_headlines):\n",
    "#     sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    \n",
    "#     for language, headlines in translated_headlines.items():\n",
    "#         # print(f\"Language: {language}\")\n",
    "#         for headline in headlines:\n",
    "#             sentiment = sentiment_pipeline(headline)\n",
    "#             print(f\"Language: {language}\")\n",
    "#             print(f\"Headline: '{headline}'\")\n",
    "#             print(f\"Label: {sentiment[0]['label']}, Score: {sentiment[0]['score']:.2f}\")\n",
    "#             print(\"---\")  # Separator for readability\n",
    "#     # return analyzed_headlines\n",
    "# # Assuming translated_headlines is your dictionary of translated headlines\n",
    "# analyze_sentiment_and_print(translated_headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38495cdc-6a28-4565-9829-43028fe4a396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2bfe6-2aec-4dd7-9cc5-809f593a46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in results: \n",
    "#     if row['label'] == 'negative':\n",
    "#         if row['score'] > 0.7:\n",
    "#             row['categ_score'] = -2\n",
    "#         else: \n",
    "#             row['categ_score'] = -1\n",
    "#     elif row['label'] == 'positive':\n",
    "#         if row['score'] > 0.7:\n",
    "#             row['categ_score'] = 3\n",
    "#         else:\n",
    "#             row['categ_score'] = 2\n",
    "#     else:\n",
    "#         row['categ_score'] = 1\n",
    "# print(results[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb51d9-3f70-4393-b131-548a375dab02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99ff4-6bef-45df-a87f-fc3a97628a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd42c3-0f53-43dc-8d10-5afafc9e95df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d0b0e-f36f-44b4-8a04-1b1d7c89acc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc920a91-5d3f-4d05-868c-8711de89916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# resultsdf = pd.DataFrame(results)\n",
    "# resultsdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebbe5f-6469-4d47-8b67-e0cb673b5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I'm going to work next: \n",
    "# Find free websites in German, French, Spanish, Russian\n",
    "# Translate all the main headlines \n",
    "# Do a sentiment analysis on the headlines using a LLM\n",
    "# calculate the avg and median scores for each language\n",
    "#COMPARE the total score for each language's website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af7c19-d0b0-4f31-91ba-1f8a08bc13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIND PYTHON library to visualize DIRECTLY on jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8d263-6f40-4bbc-9f38-2d621444f71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
